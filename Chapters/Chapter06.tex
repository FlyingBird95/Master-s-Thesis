%************************************************
\chapter{Evaluation}\label{ch:evaluation}
%************************************************
This chapter evaluates the system proposed in \autoref{ch:design}. This evaluation is based on analysing the requirements that are described in \autoref{sec:architectural_req}. Furthermore, this chapter continues the evaluation of the proposed solution by means of an experimental procedure aimed at evaluating its accuracy. This chapter ends with answering the sub questions, as proposed in \autoref{sec:research_question}.

\section{Evaluating architectural requirements}
\autoref{sec:architectural_req} describes the architectural requirements. This section provides a small overview whether the requirements are met. The numbering is consistent with the requirements in \autoref{sec:architectural_req}.

\begin{enumerate}
    \item The system has been designed in such a way that it is functional across multiple cloud providers. The system has been tested on Google Cloud Engine, while the case study operates on a private cloud.
    \item The data transmitted between nodes is not secured, as it is being send over HTTP. However, the system should be set up in a way in which only the dashboard is publicly available. As this is the case, the data is only accessible in the Grafana dashboard. This can be seen only using a username-password authentication. However, the dashboard is automatically served over HTTP. Therefore, we conclude that the data is not secure.
    \item We conclude that the system is sufficient configurable. There are configuration options for all resource prices, network traffic monitoring and node types. Therefore, the system can be tweaked to support multiple systems, each with its own characteristics. However, this is not a full proof, and we verified the system only using a demo application and at an industrial company. The solution is easy to use, as it works without any configuration as well (due to default variables).
    \item \autoref{fig:deployment} presents the overhead of the monitoring solution during the case study. It can be seen that the overall CPU utilization is low. Furthermore, \autoref{sec:eval_k} explains how the solution can be tweaked to balance between efficiency and effectiveness. We conclude that there is a balance, although this must be configured.
    \item The solution is able to collect utilization statistics on the monitored system using cAdvisor and TCPdump, as described in \autoref{sec:node}.
    \item The proposed solution is scalable, although it has limits. Using the hierarchical structure defined in \autoref{sec:architecture}, the system is able to scale to tens of super nodes and millions of nodes.
    However, the data is not aggregated at the root, so this becomes a bottleneck. The Grafana dashboard requests data from the Prometheus database, which returns each container with the corresponding info as a datapoint. As explained in the Grafana documentation \cite{grafana}, the maximum number of datapoints is $2495$. Therefore, we conclude that the proposed solution is not very scalable.
    \item The solution is elastic as it can detect new running containers and stopped containers. As a container is stopped and removed, the data persists in the database. Furthermore, in case of new resources (i.e. virtual machines) added to the system, the solution is able to automatically update the hierarchical structure (assuming the solution is configured correctly). If the new monitoring node is a supernode, than it automatically receives a number of nodes to monitor. If the new monitoring node is a normal node, it is automatically scraped by one of the available supernodes. Therefore, we conclude that the system is elastic.
    \item The solution is adaptable, as it can deal with spikes of network load. In this case, the monitoring solution has a smaller sleeping period, which leads to a decrease in the network sleeping time. However, this sleeping time is always a constant multiple of the network monitoring time. Therefore, it is not fully adaptable, as the CPU utilization of the monitored tool increases as the network traffic increases.
    \item The solution is not autonomic, as it is not able to self-manage its distributed resources by automatically reacting to unpredictable changes. Although the solution works by connecting a new monitoring node automatically to the other nodes, it cannot scale these nodes on-demand without the help of the Cloud Consumer. Therefore, we conclude that the system is not fully autonomic.
    \item The accuracy of the solution can be evaluated by determining the accuracy of the individual components. This is all described in the section below (\autoref{sec:accuracy_evaluation}). The conclusion of the accuracy evaluation can be found in \autoref{sec:accuracy_conclusion}.
\end{enumerate}

\noindent
From the items above, we conclude that the system is unsuccessful to meet all requirements. Three of them are not met, which are security, scalability and autonomicity. The other seven requirements are sufficiently met.

\section{Accuracy evaluation} \label{sec:accuracy_evaluation}
The accuracy evaluation is divided into three parts. First, the accuracy of the pricing model proposed in \autoref{sec:pricing} can be found in \autoref{sec:eval_pricing}. Second, the accuracy of the network traffic monitoring can be found in \autoref{sec:eval_k}. Third, the effectiveness of the presented information is evaluated. All three evaluations end up in an overall conclusion in \autoref{sec:accuracy_conclusion}.

\subsection{Pricing model accuracy} \label{sec:eval_pricing}
Due to the complexity of the Cloud and a large number of Cloud Providers, it is extremely complicated to develop an accurate pricing model. This section evaluates the pricing model from \autoref{eq:p}. This pricing model is a combination of three price values, and three resource values.\\

\noindent
The price values are designed in such a way that they can be changed dynamically. Therefore, the user of the monitoring tool can adjust the prices such that they fit to his pricing model. This provides a simple solution, but is based on the assumption that the prices do not change throughout the lifetime of the monitoring solution (which might be in case the Cloud Provider offers discount for longer use). Furthermore, it is also based on the assumption that there are no free resources in the beginning (e.g. the first 50 GB of outgoing network data is free) of the Cloud-based application. From the other point of view, using the flexibility of the pricing model, accurate costs can be estimated. For example, if the price per outgoing GB is $\$0.10$, and the solution has monitored 200 GB so far. The price for the model could be $\$0.075$ in case the first 50 GB is indeed free. The same trick can be applied in case the Cloud Provider offers discount for longer use. However, by continuous changing the prices manually, we can conclude that the pricing model on itself is too simplistic and therefore not very accurate.\\

\noindent
It needs to be pointed out that the model, although simplistic, is easy-to-use. According to the technical employee of the industrial company, the ``pricing is of course configurable, so it can be adjusted to be more accurate for other scenarios''. The issues from the previous paragraph are only effective when the tool is used for at least a month. \cite{infoworld} explains that ``monthly usage, saves about 10 percent over on-demand usage''. It's inaccuracy for monitoring two months is only $5\%$.

\subsection{Network traffic accuracy} \label{sec:eval_k}
This section presents the accuracy of network traffic monitoring. In order to do so, a supernode was deployed to monitor two containers. One container acting as a web-service, and one container continuously sending requests. The command for collecting all values can be found below:

\begin{lstlisting}[language=bash, caption=Docker-compose]
git clone htts://github.com/dadvisor/util
./util/accuracy.sh
\end{lstlisting}

\noindent
The idea behind this evaluation is to allow only one container to communicate with the web-service. By doing so, the total amount of data that a container sends is equivalent to its internal amount of data. Thus, the following variables should hold the same value:
\begin{itemize}
    \item \textbf{network\_container\_total}: This metric receives a value by scraping data from cAdvisor.
    \item \textbf{bytes\_send\_total}: This metrics has a key-value pair with both `src' and `dst'. This is the amount of data sent between the source container and the destination container by reading the TCPdump data.
\end{itemize}

\noindent
By analysing the rate (per-second average rate of increase) over a time period of one hour, the accuracy can be determined. This can be expressed as the ratio between the two variables above. A perfect estimation would therefore result in a value of $1$. The Prometheus query for analysing the accuracy is presented below.

\begin{verbatim}
rate(network_container_total[5m]) / on (src) 
rate(bytes_send_total[5m])
\end{verbatim}

\noindent
The evaluation has been made for different K values, as proposed in \autoref{sec:exposing_data}. The data obtained for $K = 3$ can be found in \autoref{fig:network_traffic_accuracy}. This figure shows the accuracy of the network traffic monitoring evaluation. Note that this figure shows several spikes, which is due to the difference update frequency of 
both variables. Therefore, the minimum and maximum value represent the boundaries of the accuracy. Furthermore, the accuracy is summarized by computing the average. The results for different K values can be found in \autoref{tab:accuracy_results}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{gfx/traffic_network_accuracy}
    \caption{Network traffic accuracy}
    \label{fig:network_traffic_accuracy}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{r|rrr|r}
        \multirow{2}{*}{K-value} & \multicolumn{3}{c|}{Accuracy} & \multirow{2}{*}{CPU Usage} \\
        & Min & Max & average & \\ \hline        
        0 & 0.53& 1.22& 1.10& 9\% \\
        3 & 0.52& 1.04& 1.00& 4\% \\
        6 & 0.95& 1.74& 1.66& 3\% \\
        9 & 0.53& 1.29& 1.05& 3\% \\
        12& 0.93& 1.33& 1.10& 2\% \\
        15& 0.51& 1.11& 1.01& 2\% \\        
    \end{tabular}
    \caption{Network traffic accuracy results for different K-values}
    \label{tab:accuracy_results}
\end{table}

\subsection{Presented information accuracy}

\subsection{Conclusion} \label{sec:accuracy_conclusion}
From \autoref{tab:accuracy_results} it can be concluded that there is not much variation in the accuracy across different K-values. A reason for this is the fact that the network communication is not enough randomized. Thus, with a constant flow of traffic, the prediction of amount of network communication is too trivial.\\

\noindent
What can be concluded from this table, is the fact that the CPU usage drops if the K-value increases. This is due to a longer sleeping period. However, this number is not very accurate, as the analyses is performed on a single node. Therefore, the overhead of communicating with other nodes is not evaluated.